{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5ZMknKeHSG8CiALiuC2Kh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahuthu/IBM_data_science/blob/main/file_formats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMtSd0I-5skC"
      },
      "outputs": [],
      "source": [
        "#Reading file formats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import piplite\n",
        "await piplite.install(['seaborn', 'lxml', 'openpyxl'])\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "OEK96KSO8SQ6",
        "outputId": "ad547ccd-5e9f-40f2-8b1c-2307afa53dde"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5991ac757904>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpiplite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mpiplite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seaborn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'openpyxl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'piplite'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyodide.http import pyfetch\n",
        "\n",
        "filename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/addresses.csv\"\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())\n",
        "\n",
        "await download(filename, \"addresses.csv\")\n",
        "\n",
        "df = pd.read_csv(\"addresses.csv\", header=None)"
      ],
      "metadata": {
        "id": "P8QF-mK18Iri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding column name to the DataFrame\n",
        "df.columns =['First Name', 'Last Name', 'Location ', 'City','State','Area Code']\n",
        "df[\"First Name\"]\n",
        "df = df[['First Name', 'Last Name', 'Location ', 'City','State','Area Code']]\n",
        "df"
      ],
      "metadata": {
        "id": "VNeUTtna9CdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loc() : loc() is label based data selecting method which means that we have to pass the name of the row or column which we want to select.\n",
        "# To select the first row\n",
        "df.loc[0]\n",
        "# To select the 0th,1st and 2nd row of \"First Name\" column only\n",
        "df.loc[[0,1,2], \"First Name\" ]"
      ],
      "metadata": {
        "id": "vASPuPi7_JeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, let's see how to use .iloc for selecting rows from our DataFrame.\n",
        "\n",
        "#**iloc() : iloc() is a indexed based selecting method which means that we have to pass integer index in the method to select specific row/column.**\n"
      ],
      "metadata": {
        "id": "i-69wUYG_l5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To select the 0th,1st and 2nd row of \"First Name\" column only\n",
        "df.iloc[[0,1,2], 0]"
      ],
      "metadata": {
        "id": "CCYxyxr2_wAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])\n",
        "df"
      ],
      "metadata": {
        "id": "nSQujLEYDcQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform Function in Pandas\n",
        "#Python's Transform function returns a self-produced dataframe with transformed values after applying the function specified in its parameter."
      ],
      "metadata": {
        "id": "FUOagrfZD-F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying the transform function\n",
        "df = df.transform(func = lambda x : x + 10)\n",
        "df"
      ],
      "metadata": {
        "id": "j4xM8rEtEWv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = df.transform(func = ['sqrt'])\n",
        "result"
      ],
      "metadata": {
        "id": "M6o3OcE0EagS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Writing JSON to a File\n",
        "#This is usually called serialization. It is the process of converting an object into\n",
        "#a special format which is suitable for transmitting over the network or storing in file or database.\n",
        "\n",
        "#To handle the data flow in a file, the JSON library in Python uses the dump() or dumps() function\n",
        "#to convert the Python objects into their respective JSON object. This makes it easy to write data to files.\n",
        "\n"
      ],
      "metadata": {
        "id": "g3VWvkCOEkuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "person = {\n",
        "    'first_name' : 'Mark',\n",
        "    'last_name' : 'abc',\n",
        "    'age' : 27,\n",
        "    'address': {\n",
        "        \"streetAddress\": \"21 2nd Street\",\n",
        "        \"city\": \"New York\",\n",
        "        \"state\": \"NY\",\n",
        "        \"postalCode\": \"10021-3100\"\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "brimvDjwFl2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#serialization using dump() function\n",
        "#json.dump() method can be used for writing to JSON file.\n",
        "\n",
        "#Syntax: json.dump(dict, file_pointer)\n",
        "\n",
        "#Parameters:\n",
        "\n",
        "#dictionary name of the dictionary which should be converted to JSON object.\n",
        "#file pointer pointer of the file opened in write or append mode."
      ],
      "metadata": {
        "id": "Xr8xV5KcFrGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('person.json', 'w') as f:  # writing JSON object\n",
        "    json.dump(person, f)"
      ],
      "metadata": {
        "id": "GO-gOKwNG--R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#serialization using dumps() function\n",
        "#json.dumps() that helps in converting a dictionary to a JSON object.\n",
        "\n",
        "#It takes two parameters:\n",
        "\n",
        "#dictionary- name of the dictionary which should be converted to JSON object.\n",
        "#indent- defines the number of units for indentation\n",
        "# Serializing json"
      ],
      "metadata": {
        "id": "B3DDNpFeHFTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_object = json.dumps(person, indent = 4)\n",
        "\n",
        "# Writing to sample.json\n",
        "with open(\"sample.json\", \"w\") as outfile:\n",
        "    outfile.write(json_object)"
      ],
      "metadata": {
        "id": "pNgcuqOqHUCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(json_object)"
      ],
      "metadata": {
        "id": "JNNy82N4HXNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading JSON to a File\n",
        "#This process is usually called Deserialization - it is the reverse of serialization. It converts the special format returned by the serialization back into a usable object.\n",
        "\n",
        "#Using json.load()\n",
        "#The JSON package has json.load() function that loads the json content from a json file into a dictionary.\n",
        "\n",
        "#It takes one parameter:\n",
        "\n",
        "#File pointer : A file pointer that points to a JSON file."
      ],
      "metadata": {
        "id": "NXVEhlomH8Tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "with open('sample.json', 'r') as openfile:\n",
        "\n",
        "    # Reading from json file\n",
        "    json_object = json.load(openfile)\n",
        "\n",
        "print(json_object)\n",
        "print(type(json_object))"
      ],
      "metadata": {
        "id": "H48MbUScIBke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the data from XLSX file¶\n"
      ],
      "metadata": {
        "id": "MVOR2COaIR3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "lS-S3jolAE0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/file_example_XLSX_10.xlsx\"\n",
        "async def download(url, filename):\n",
        "  response = await pyfetch(url)\n",
        "  if response.status == 200:\n",
        "    with open(filename, \"wb\") as f:\n",
        "      f.write(await response.bytes())\n",
        "\n",
        "await download(filename, \"file_example_XLSX_10.xlsx\")\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"file_example_XLSX_10.xlsx\")\n",
        "df"
      ],
      "metadata": {
        "id": "vNpk17m2AMGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Writing with xml.etree.ElementTree\n",
        "#The xml.etree.ElementTree module comes built-in with Python. It provides functionality for parsing and creating XML documents.\n",
        "#ElementTree represents the XML document as a tree. We can move across the document using nodes which are elements and sub-elements of the XML file."
      ],
      "metadata": {
        "id": "WwQZelmwGLI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# create the file structure\n",
        "employee = ET.Element('employee')\n",
        "details = ET.SubElement(employee, 'details')\n",
        "first = ET.SubElement(details, 'firstname')\n",
        "second = ET.SubElement(details, 'lastname')\n",
        "third = ET.SubElement(details, 'age')\n",
        "first.text = 'Shiv'\n",
        "second.text = 'Mishra'\n",
        "third.text = '23'\n",
        "\n",
        "# create a new XML file with the results\n",
        "mydata1 = ET.ElementTree(employee)\n",
        "# myfile = open(\"items2.xml\", \"wb\")\n",
        "# myfile.write(mydata)\n",
        "with open(\"new_sample.xml\", \"wb\") as files:\n",
        "    mydata1.write(files)"
      ],
      "metadata": {
        "id": "qhq-3M3IE2fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as etree\n",
        "\n",
        "filename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/Sample-employee-XML-file.xml\"\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())\n",
        "\n",
        "await download(filename, \"Sample-employee-XML-file.xml\")"
      ],
      "metadata": {
        "id": "ebS_7lt2E9G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading with xml.etree.ElementTree\n",
        "#You would need to firstly parse an XML file and create a list of columns for data frame, then extract useful information from the XML file and add to a pandas data frame."
      ],
      "metadata": {
        "id": "NiqxC9-WFS9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as etree\n",
        "\n",
        "filename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/Sample-employee-XML-file.xml\"\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())\n",
        "\n",
        "await download(filename, \"Sample-employee-XML-file.xml\")"
      ],
      "metadata": {
        "id": "YHEXAqIcFRHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = etree.parse(\"Sample-employee-XML-file.xml\")\n",
        "\n",
        "root = tree.getroot()\n",
        "columns = [\"firstname\", \"lastname\", \"title\", \"division\", \"building\",\"room\"]\n",
        "\n",
        "datatframe = pd.DataFrame(columns = columns)\n",
        "\n",
        "for node in root:\n",
        "\n",
        "    firstname = node.find(\"firstname\").text\n",
        "\n",
        "    lastname = node.find(\"lastname\").text\n",
        "\n",
        "    title = node.find(\"title\").text\n",
        "\n",
        "    division = node.find(\"division\").text\n",
        "\n",
        "    building = node.find(\"building\").text\n",
        "\n",
        "    room = node.find(\"room\").text\n",
        "\n",
        "    datatframe = datatframe.append(pd.Series([firstname, lastname, title, division, building, room], index = columns), ignore_index = True)"
      ],
      "metadata": {
        "id": "iqFU7BXVGYCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datatframe"
      ],
      "metadata": {
        "id": "XDKrJowMGuj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_xml(\"Sample-employee-XML-file.xml\", xpath=\"/employees/details\")"
      ],
      "metadata": {
        "id": "tBijyf01GzO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datatframe.to_csv(\"employee.csv\", index=False)"
      ],
      "metadata": {
        "id": "Jkmx_g9K8SXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the Image file\n",
        "#Python supports very powerful tools when it comes to image processing. Let's see how to process the images using the PIL library.\n",
        "\n",
        "#PIL is the Python Imaging Library which provides the python interpreter with image editing capabilities."
      ],
      "metadata": {
        "id": "3ZhtUcq09Si2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing PIL\n",
        "from PIL import Image\n",
        "\n",
        "# Uncomment if running locally\n",
        "# import urllib.request\n",
        "# urllib.request.urlretrieve(\"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/dog-puppy-on-garden-royalty-free-image-1586966191.jpg\", \"dog.jpg\")\n",
        "\n",
        "filename = \"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/dog-puppy-on-garden-royalty-free-image-1586966191.jpg\"\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())\n",
        "\n",
        "await download(filename, \"dog.jpg\")"
      ],
      "metadata": {
        "id": "vgBRXmWL88QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read image\n",
        "img = Image.open('dog.jpg')\n",
        "\n",
        "# Output Images\n",
        "display(img)"
      ],
      "metadata": {
        "id": "_JV1-Uus9WpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Analysis\n",
        "#This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases.\n",
        "#The objective of the dataset is to diagnostically predict whether or not a patient has diabetes,\n",
        "#based on certain diagnostic measurements included in the dataset. Several constraints were placed on the\n",
        "# selection of these instances from a larger database. In particular, all patients here are females at least 21 years of age of Pima Indian heritage.\n",
        "\n",
        "#Content: The datasets consists of several medical predictor variables and one target variable, Outcome.\n",
        "# Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on"
      ],
      "metadata": {
        "id": "LfyRw3d8-bxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas library\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "htaoNSPL-FIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/diabetes.csv\"\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())\n",
        "\n",
        "await download(filename, \"diabetes.csv\")\n",
        "df = pd.read_csv(\"diabetes.csv\")"
      ],
      "metadata": {
        "id": "fIYdZIXP-GSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The first 5 rows of the dataframe\")\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "CkGRArU4-yWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "qdQmjqH8-2GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "a15W23Kw-5tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_data = df.isnull()\n",
        "missing_data.head(5)"
      ],
      "metadata": {
        "id": "9WPmyuLY_A0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in missing_data.columns.values.tolist():\n",
        "    print(column)\n",
        "    print (missing_data[column].value_counts())\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "8nbZD0i5_FNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Correct data format\n",
        "#Check all data is in the correct format (int, float, text or other).\n",
        "\n",
        "#In Pandas, we use\n",
        "#dtype() to check the data type\n",
        "#astype() to change the data type\n",
        "#Numerical variables should have type 'float' or 'int'."
      ],
      "metadata": {
        "id": "E1nMtpEJ_QG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "QQ6ttHtG_aUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "jHHcZMW3_fgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels= 'Diabetic','Not Diabetic'\n",
        "plt.pie(df['Outcome'].value_counts(),labels=labels,autopct='%0.02f%%')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lI6q1GnM_itS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}